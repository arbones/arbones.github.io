[
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "Repository Overview: Quarto Blog",
    "section": "",
    "text": "This repository contains the source code for a personal blog built using the Quarto framework.\n\n\n\nQuarto Framework: The core of the project is Quarto, a publishing system for scientific and technical content. The main configuration is in _quarto.yml, which defines the website’s structure, title (“ADIPOFAT”), navigation, and output directory (docs).\nContent:\n\nBlog posts are written in Quarto Markdown (.qmd) and reside in the posts/ directory.\nThe main page, index.qmd, is configured to list all posts from the posts directory.\nabout.qmd is the “About” page.\n\nOutput: The entire project is rendered into a static HTML website in the docs/ directory. This folder is what is published and made publicly available.\nStyling: The site uses the “cosmo” theme and includes custom styling from styles.css.\nDevelopment: The presence of a .Rproj file indicates that this project is likely developed using the RStudio IDE.\n\nIn short, it’s a Quarto project that takes .qmd files, and renders them into a blog website within the docs/ directory.\n\n\n\n\n\nADIPOFAT is a scientific blog focused on the intersection of omics sciences, spatial biology, and data science. The blog serves as an open research notebook where the author (JM) shares notes, tutorials, reflections, and occasional digressions from their work in computational biology.\nThe primary goal is to create a practical resource for fellow researchers and students navigating the complexities of modern biological data analysis. The blog documents the learning process openly, covering both successes and challenges in real-world research contexts.\nThis blog is a component of broader research activities. For comprehensive information about lab projects, publications, and collaborations, readers are directed to the lab’s main page.\n\n\n\nThe blog is written for: * Computational biologists working with high-dimensional omics data * Graduate students and postdocs learning bioinformatics and spatial analysis techniques * Wet-lab researchers transitioning to computational work * Data scientists interested in biological applications * Clinicians and translational researchers exploring omics technologies\n\n\n\nThe blog covers several interconnected themes:\n\n\nTopics include genomics, transcriptomics, proteomics, metabolomics, and their computational analysis. The blog emphasizes: * Single-cell genomics methods and tools * Spatial transcriptomics and imaging-based approaches * Multi-omics integration strategies * Best practices for data analysis pipelines\n\n\n\nCritical evaluations of computational tools, software ecosystems, and analytical workflows. This includes: * Community resources (e.g., curated repositories like awesome-single-cell) * Data management and reproducible research practices * AI-assisted scientific writing and analysis workflows * Integration of tools like Quarto, R, Python, and Claude Code\n\n\n\nPractical guidance on avoiding common pitfalls and implementing robust analytical approaches: * Data normalization and preprocessing methods * Statistical considerations in high-throughput experiments * Critical analysis of standard pipelines and when they fail * Version control and computational reproducibility\n\n\n\nThe blog doesn’t shy away from examining problems in current research practices: * Dangers of inappropriate tool usage (e.g., Excel in scientific research) * Hidden assumptions in standard analysis methods * Cases where borrowed methods fail in new contexts * Technical debt in scientific computing\n\n\n\n\nAs of January 2026, the blog contains five posts spanning introductory content, critical commentary, technical tutorials, and deep technical analyses:\n\n“Hello World from Omic Science!” (September 2025)\n\nCategories: omics, bioinformatics, introduction\nContent: Introductory post establishing the blog’s scope and future direction. Outlines planned coverage of genomics, transcriptomics, proteomics, metabolomics, bioinformatics tools, reproducible workflows, and multi-omics integration.\n\n“The Excel-pocalypse: Where Good Research Goes to Die in a Spreadsheet” (September 2025)\n\nCategories: excel, data-wrangling\nContent: Critical examination of Excel’s role in scientific disasters. Documents real cases including gene name autoconversion errors affecting 20-30% of genomics papers and the UK COVID-19 testing data loss (15,841 cases) caused by Excel row limits. Argues for proper databases and programming tools over spreadsheets for research data.\n\n“Awesome-Single-Cell: Navigating the Single-Cell Landscape” (September 2025)\n\nCategories: SingleCell, GitHub, Repository\nContent: Review of the community-curated awesome-single-cell GitHub repository (3.5k+ stars, 180+ contributors). Describes how this resource organizes tools for RNA-seq, ATAC-seq, spatial transcriptomics, multi-omics integration, and provides tutorials and benchmarking studies for the single-cell genomics field.\n\n“AI-Enhanced Scientific Writing: Transforming Quarto Analyses into Publication-Ready Narratives with Claude Code” (December 2025)\n\nCategories: Claude, AI, Writing, Quarto\nContent: Comprehensive tutorial on the author’s workflow for using Claude Code to convert computational analyses into scientific prose. Details the technical architecture including CLAUDE.md project instructions, .context/ directory with writing style specifications (Oliver Smithies style guide), and the quarto-narrative-skill system. Explains how the workflow reads source .qmd files, rendered .html outputs, and experimental context to generate publication-ready narratives while preserving scientific accuracy and consistent voice.\n\n“Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)” (January 2026)\n\nCategories: SingleCell, SRT, Normalization\nContent: Technical deep-dive into normalization methods for spatial transcriptomics data. Examines how standard single-cell RNA-seq normalization methods (LogNormalize, SCTransform, scran, etc.) can inadvertently remove genuine biological signals from spatial data due to spatial organization of tissues. Discusses the assumption that library size variation is purely technical, which fails when cell density and RNA content vary systematically across anatomical structures. Provides practical recommendations for spatial data analysis and introduces spatially-aware methods like SpaNorm.\n\n\n\n\n\nThe blog employs an authoritative but accessible tone, balancing technical depth with readability. Posts frequently: * Use concrete examples and real-world case studies * Cite primary literature and benchmarking studies * Provide practical recommendations alongside theoretical discussion * Acknowledge complexity and trade-offs rather than presenting oversimplified solutions * Link to external resources, documentation, and further reading\nThe writing reflects a researcher’s perspective: critical, evidence-based, and focused on what actually works in practice rather than what should work in theory.\n\n\n\n\nThis blog is optimized for deployment on GitHub Pages. The configuration for GitHub Pages can be found at Configuring a publishing source for your GitHub Pages site.\nThe key for this project is that the docs/ directory is the publishing source for GitHub Pages.\n\n\n\nThe visual appearance of the blog can be modified programmatically through the following files:\n\n\nThis file controls the overall theme of the blog. The format section is key for this:\nformat:\n  html:\n    theme:\n      - cosmo\n      - brand\n    css: styles.css\n\ntheme: This specifies the Bootstrap theme to use. This blog uses the cosmo theme. You can find more themes on the Quarto HTML formats page. The blog also uses a custom theme called brand.\ncss: This points to a custom CSS file, styles.css, for further styling modifications.\n\nTo change the main theme, you can replace cosmo with another theme name.\n\n\n\nThis file is used for custom CSS rules that override the default theme. Currently, this file is empty.\nTo modify the aesthetics, you can add any valid CSS rules to this file. For example, to change the color of the titles, you could add:\nh1 {\n  color: #333;\n}\nAny changes in this file will be reflected in the final rendered website.\n\n\n\n\nThe blog posts are organized in the posts directory, following a specific structure that is common for Quarto blogs.\n\n\nEach post is contained within its own subdirectory inside posts. This allows for better organization of content, especially when a post includes images or other files.\nposts/\n├── 00.hello_world/\n│   ├── index.qmd\n│   └── thumbnail.png\n├── 01.excel/\n│   ├── index.qmd\n│   └── thumbnail.png\n├── ...\n└── _metadata.yml\n\nposts/: The main directory containing all blog posts.\n00.hello_world/, 01.excel/, etc.: Subdirectories for individual posts.\nindex.qmd: The main content file for each post, written in Quarto Markdown.\nthumbnail.png, etc.: Images or other assets associated with a post are kept within the same subdirectory.\n\n\n\n\nEach index.qmd file starts with a YAML block that contains metadata for the post. This metadata controls the title, author, date, categories, and other post-specific options.\nExample from posts/00.hello_world/index.qmd:\n---\ntitle: \"Hello World...\"\nsubtitle: \"...from Omic Science!\"\nlayout: post\ndate: 2025-09-18\nauthor: \"JM\"\nimage: \"thumbnail.png\"\ncategories: [omics, bioinformatics, introduction]\n---\n\n\n\nThe posts/_metadata.yml file defines options that apply to all posts within the posts directory. This avoids having to repeat the same configuration in each post’s YAML block.\nContents of posts/_metadata.yml:\n# options specified here will apply to all posts in this folder\n\n# freeze computational output\n# (see https://quarto.org/docs/projects/code-execution.html#freeze)\nfreeze: true\n\n# Enable banner style title blocks\ntitle-block-banner: true\n\nfreeze: true: This option tells Quarto to avoid re-executing code chunks when rendering the site, unless the code has changed. This speeds up the rendering process.\ntitle-block-banner: true: This enables a banner-style title block for all posts.\n\nThis structure allows for a clean and organized way to manage blog posts and their associated files."
  },
  {
    "objectID": "CLAUDE.html#architecture",
    "href": "CLAUDE.html#architecture",
    "title": "Repository Overview: Quarto Blog",
    "section": "",
    "text": "Quarto Framework: The core of the project is Quarto, a publishing system for scientific and technical content. The main configuration is in _quarto.yml, which defines the website’s structure, title (“ADIPOFAT”), navigation, and output directory (docs).\nContent:\n\nBlog posts are written in Quarto Markdown (.qmd) and reside in the posts/ directory.\nThe main page, index.qmd, is configured to list all posts from the posts directory.\nabout.qmd is the “About” page.\n\nOutput: The entire project is rendered into a static HTML website in the docs/ directory. This folder is what is published and made publicly available.\nStyling: The site uses the “cosmo” theme and includes custom styling from styles.css.\nDevelopment: The presence of a .Rproj file indicates that this project is likely developed using the RStudio IDE.\n\nIn short, it’s a Quarto project that takes .qmd files, and renders them into a blog website within the docs/ directory."
  },
  {
    "objectID": "CLAUDE.html#blog-content-and-themes",
    "href": "CLAUDE.html#blog-content-and-themes",
    "title": "Repository Overview: Quarto Blog",
    "section": "",
    "text": "ADIPOFAT is a scientific blog focused on the intersection of omics sciences, spatial biology, and data science. The blog serves as an open research notebook where the author (JM) shares notes, tutorials, reflections, and occasional digressions from their work in computational biology.\nThe primary goal is to create a practical resource for fellow researchers and students navigating the complexities of modern biological data analysis. The blog documents the learning process openly, covering both successes and challenges in real-world research contexts.\nThis blog is a component of broader research activities. For comprehensive information about lab projects, publications, and collaborations, readers are directed to the lab’s main page.\n\n\n\nThe blog is written for: * Computational biologists working with high-dimensional omics data * Graduate students and postdocs learning bioinformatics and spatial analysis techniques * Wet-lab researchers transitioning to computational work * Data scientists interested in biological applications * Clinicians and translational researchers exploring omics technologies\n\n\n\nThe blog covers several interconnected themes:\n\n\nTopics include genomics, transcriptomics, proteomics, metabolomics, and their computational analysis. The blog emphasizes: * Single-cell genomics methods and tools * Spatial transcriptomics and imaging-based approaches * Multi-omics integration strategies * Best practices for data analysis pipelines\n\n\n\nCritical evaluations of computational tools, software ecosystems, and analytical workflows. This includes: * Community resources (e.g., curated repositories like awesome-single-cell) * Data management and reproducible research practices * AI-assisted scientific writing and analysis workflows * Integration of tools like Quarto, R, Python, and Claude Code\n\n\n\nPractical guidance on avoiding common pitfalls and implementing robust analytical approaches: * Data normalization and preprocessing methods * Statistical considerations in high-throughput experiments * Critical analysis of standard pipelines and when they fail * Version control and computational reproducibility\n\n\n\nThe blog doesn’t shy away from examining problems in current research practices: * Dangers of inappropriate tool usage (e.g., Excel in scientific research) * Hidden assumptions in standard analysis methods * Cases where borrowed methods fail in new contexts * Technical debt in scientific computing\n\n\n\n\nAs of January 2026, the blog contains five posts spanning introductory content, critical commentary, technical tutorials, and deep technical analyses:\n\n“Hello World from Omic Science!” (September 2025)\n\nCategories: omics, bioinformatics, introduction\nContent: Introductory post establishing the blog’s scope and future direction. Outlines planned coverage of genomics, transcriptomics, proteomics, metabolomics, bioinformatics tools, reproducible workflows, and multi-omics integration.\n\n“The Excel-pocalypse: Where Good Research Goes to Die in a Spreadsheet” (September 2025)\n\nCategories: excel, data-wrangling\nContent: Critical examination of Excel’s role in scientific disasters. Documents real cases including gene name autoconversion errors affecting 20-30% of genomics papers and the UK COVID-19 testing data loss (15,841 cases) caused by Excel row limits. Argues for proper databases and programming tools over spreadsheets for research data.\n\n“Awesome-Single-Cell: Navigating the Single-Cell Landscape” (September 2025)\n\nCategories: SingleCell, GitHub, Repository\nContent: Review of the community-curated awesome-single-cell GitHub repository (3.5k+ stars, 180+ contributors). Describes how this resource organizes tools for RNA-seq, ATAC-seq, spatial transcriptomics, multi-omics integration, and provides tutorials and benchmarking studies for the single-cell genomics field.\n\n“AI-Enhanced Scientific Writing: Transforming Quarto Analyses into Publication-Ready Narratives with Claude Code” (December 2025)\n\nCategories: Claude, AI, Writing, Quarto\nContent: Comprehensive tutorial on the author’s workflow for using Claude Code to convert computational analyses into scientific prose. Details the technical architecture including CLAUDE.md project instructions, .context/ directory with writing style specifications (Oliver Smithies style guide), and the quarto-narrative-skill system. Explains how the workflow reads source .qmd files, rendered .html outputs, and experimental context to generate publication-ready narratives while preserving scientific accuracy and consistent voice.\n\n“Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)” (January 2026)\n\nCategories: SingleCell, SRT, Normalization\nContent: Technical deep-dive into normalization methods for spatial transcriptomics data. Examines how standard single-cell RNA-seq normalization methods (LogNormalize, SCTransform, scran, etc.) can inadvertently remove genuine biological signals from spatial data due to spatial organization of tissues. Discusses the assumption that library size variation is purely technical, which fails when cell density and RNA content vary systematically across anatomical structures. Provides practical recommendations for spatial data analysis and introduces spatially-aware methods like SpaNorm.\n\n\n\n\n\nThe blog employs an authoritative but accessible tone, balancing technical depth with readability. Posts frequently: * Use concrete examples and real-world case studies * Cite primary literature and benchmarking studies * Provide practical recommendations alongside theoretical discussion * Acknowledge complexity and trade-offs rather than presenting oversimplified solutions * Link to external resources, documentation, and further reading\nThe writing reflects a researcher’s perspective: critical, evidence-based, and focused on what actually works in practice rather than what should work in theory."
  },
  {
    "objectID": "CLAUDE.html#github-pages-optimization",
    "href": "CLAUDE.html#github-pages-optimization",
    "title": "Repository Overview: Quarto Blog",
    "section": "",
    "text": "This blog is optimized for deployment on GitHub Pages. The configuration for GitHub Pages can be found at Configuring a publishing source for your GitHub Pages site.\nThe key for this project is that the docs/ directory is the publishing source for GitHub Pages."
  },
  {
    "objectID": "CLAUDE.html#aesthetics-and-customization",
    "href": "CLAUDE.html#aesthetics-and-customization",
    "title": "Repository Overview: Quarto Blog",
    "section": "",
    "text": "The visual appearance of the blog can be modified programmatically through the following files:\n\n\nThis file controls the overall theme of the blog. The format section is key for this:\nformat:\n  html:\n    theme:\n      - cosmo\n      - brand\n    css: styles.css\n\ntheme: This specifies the Bootstrap theme to use. This blog uses the cosmo theme. You can find more themes on the Quarto HTML formats page. The blog also uses a custom theme called brand.\ncss: This points to a custom CSS file, styles.css, for further styling modifications.\n\nTo change the main theme, you can replace cosmo with another theme name.\n\n\n\nThis file is used for custom CSS rules that override the default theme. Currently, this file is empty.\nTo modify the aesthetics, you can add any valid CSS rules to this file. For example, to change the color of the titles, you could add:\nh1 {\n  color: #333;\n}\nAny changes in this file will be reflected in the final rendered website."
  },
  {
    "objectID": "CLAUDE.html#blog-post-structure",
    "href": "CLAUDE.html#blog-post-structure",
    "title": "Repository Overview: Quarto Blog",
    "section": "",
    "text": "The blog posts are organized in the posts directory, following a specific structure that is common for Quarto blogs.\n\n\nEach post is contained within its own subdirectory inside posts. This allows for better organization of content, especially when a post includes images or other files.\nposts/\n├── 00.hello_world/\n│   ├── index.qmd\n│   └── thumbnail.png\n├── 01.excel/\n│   ├── index.qmd\n│   └── thumbnail.png\n├── ...\n└── _metadata.yml\n\nposts/: The main directory containing all blog posts.\n00.hello_world/, 01.excel/, etc.: Subdirectories for individual posts.\nindex.qmd: The main content file for each post, written in Quarto Markdown.\nthumbnail.png, etc.: Images or other assets associated with a post are kept within the same subdirectory.\n\n\n\n\nEach index.qmd file starts with a YAML block that contains metadata for the post. This metadata controls the title, author, date, categories, and other post-specific options.\nExample from posts/00.hello_world/index.qmd:\n---\ntitle: \"Hello World...\"\nsubtitle: \"...from Omic Science!\"\nlayout: post\ndate: 2025-09-18\nauthor: \"JM\"\nimage: \"thumbnail.png\"\ncategories: [omics, bioinformatics, introduction]\n---\n\n\n\nThe posts/_metadata.yml file defines options that apply to all posts within the posts directory. This avoids having to repeat the same configuration in each post’s YAML block.\nContents of posts/_metadata.yml:\n# options specified here will apply to all posts in this folder\n\n# freeze computational output\n# (see https://quarto.org/docs/projects/code-execution.html#freeze)\nfreeze: true\n\n# Enable banner style title blocks\ntitle-block-banner: true\n\nfreeze: true: This option tells Quarto to avoid re-executing code chunks when rendering the site, unless the code has changed. This speeds up the rendering process.\ntitle-block-banner: true: This enables a banner-style title block for all posts.\n\nThis structure allows for a clean and organized way to manage blog posts and their associated files."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BLOG",
    "section": "",
    "text": "LinkedIn Newsletter Summaries for “Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Your Spatial Transcriptomics Analysis Might Be Lying to You (I)\n\n\nA deep dive into normalization methods and why the tools we borrowed from single-cell RNA-seq don’t always work for spatial data\n\n\n\nSingleCell\n\nSRT\n\nNormalization\n\n\n\n\n\n\n\n\n\nJan 4, 2026\n\n\nJM\n\n\n\n\n\n\n\n\n\n\n\n\nAI-Enhanced Scientific Writing\n\n\nTransforming Quarto Analyses into Publication-Ready Narratives with Claude code\n\n\n\nClaude\n\nAI\n\nWriting\n\nQuarto\n\n\n\n\n\n\n\n\n\nDec 20, 2025\n\n\nJM\n\n\n\n\n\n\n\n\n\n\n\n\nAwesome-Single-Cell\n\n\nNavigating the Single-Cell Landscape: An Essential Community Resource\n\n\n\nSingleCell\n\nGitHub\n\nRepository\n\n\n\n\n\n\n\n\n\nSep 23, 2025\n\n\nJM\n\n\n\n\n\n\n\n\n\n\n\n\nThe Excel-pocalypse\n\n\nWhere Good Research Goes to Die in a Spreadsheet\n\n\n\nexcel\n\ndata-wrangling\n\n\n\n\n\n\n\n\n\nSep 20, 2025\n\n\nJM\n\n\n\n\n\n\n\n\n\n\n\n\nHello World…\n\n\n…from Omic Science!\n\n\n\nomics\n\nbioinformatics\n\nintroduction\n\n\n\n\n\n\n\n\n\nSep 18, 2025\n\n\nJM\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/01.excel/index.html",
    "href": "posts/01.excel/index.html",
    "title": "The Excel-pocalypse",
    "section": "",
    "text": "Spreadsheets look simple, friendly, and universal. But history has shown again and again: when important science is entrusted to a spreadsheet, disaster is never far away."
  },
  {
    "objectID": "posts/01.excel/index.html#copy-paste-catastrophes",
    "href": "posts/01.excel/index.html#copy-paste-catastrophes",
    "title": "The Excel-pocalypse",
    "section": "Copy-Paste Catastrophes",
    "text": "Copy-Paste Catastrophes\nSpreadsheets don’t come with version control. In one economics study, copy-pasted formulas led to the exclusion of entire rows of countries. The result was the infamous Reinhart & Rogoff paper that claimed high debt stifled economic growth, a conclusion later shown to hinge on Excel formula errors. That single spreadsheet mistake shaped years of austerity policy."
  },
  {
    "objectID": "posts/01.excel/index.html#the-silent-assassin-hidden-sorts-and-filters",
    "href": "posts/01.excel/index.html#the-silent-assassin-hidden-sorts-and-filters",
    "title": "The Excel-pocalypse",
    "section": "The Silent Assassin: Hidden Sorts and Filters",
    "text": "The Silent Assassin: Hidden Sorts and Filters\nCountless researchers have discovered too late that sorting a single column without including the rest of the dataset destroys data integrity. One mis-click, and patient IDs no longer match blood samples, timepoints, or genotypes. The danger is compounded because Excel rarely warns you. Instead, it happily reorders cells, leaving your dataset looking fine but fatally scrambled."
  },
  {
    "objectID": "posts/01.excel/index.html#the-fragility-of-a-single-cell",
    "href": "posts/01.excel/index.html#the-fragility-of-a-single-cell",
    "title": "The Excel-pocalypse",
    "section": "The Fragility of a Single Cell",
    "text": "The Fragility of a Single Cell\nUnlike databases, spreadsheets don’t enforce data types, relationships, or consistency. A stray keystroke, an overwritten formula, or a decimal separator flipped from “,” to “.” can alter results with no trace left behind. Worse, once the error propagates through linked sheets, nobody can reconstruct what the “true” numbers were."
  },
  {
    "objectID": "posts/01.excel/index.html#has-already-happened",
    "href": "posts/01.excel/index.html#has-already-happened",
    "title": "The Excel-pocalypse",
    "section": "Has already happened",
    "text": "Has already happened\nThe Excel-pocalypse isn’t coming. It already happened. And it will keep happening until researchers stop mistaking a consumer office tool for a scientific data platform.\n\nGene Symbols vs. Calendar Dominance\nImagine you name a gene SEPT2 (Septin-2). Or MARCH1. Breathless lab techs think they wrote science. But Excel thinks it’s September 2. Or March 1. Silent autoconversion. By the time anyone notices, databases have been invaded by dates. A 2016 study found about 20% of genomics papers with supplementary Excel gene lists have such errors. (BioMed Central)\nTen years later: worse. A 2021 follow-up showed that gene name errors have increased, affecting ~30.9% of papers with Excel gene lists in a sample from 2014-2020. (PLOS)\n\n\n\nThe Great Covid Spreadsheet Slip\nPicture this: Public Health England (PHE) is collecting COVID-19 positive test results from many labs. Some send huge CSVs. Then, someone opens them in Excel, using or converting to old “.xls” format. That format can only hold ~65,536 rows. Any excess? Deleted from view. Just… gone.\nThousands of positive tests (15,841, in fact) were not included in official figures during a period in 2020. Tracing people who should have been told to isolate? Delayed. Infection chains kept growing. (The Guardian)"
  },
  {
    "objectID": "posts/01.excel/index.html#the-broader-lesson",
    "href": "posts/01.excel/index.html#the-broader-lesson",
    "title": "The Excel-pocalypse",
    "section": "The Broader Lesson",
    "text": "The Broader Lesson\nExcel has its place: small tables, quick summaries, maybe a plot for a presentation. But for storing raw research data or doing serious analysis, it is a loaded gun pointed at your results. Databases, R, Python, and specialized bioinformatics tools exist for a reason.\nEvery dataset that matters deserves version control, audit trails, reproducibility, and transparency. None of these live in Excel. And yet, in labs across the world, the fate of experiments, patients, and entire fields of knowledge continues to balance precariously on a spreadsheet."
  },
  {
    "objectID": "posts/02.awesomeSC/index.html",
    "href": "posts/02.awesomeSC/index.html",
    "title": "Awesome-Single-Cell",
    "section": "",
    "text": "Single-cell genomics is growing at breakneck speed, generating massive datasets that demand specialized analysis. The challenge isn’t just the biology, it’s keeping up with the ever-expanding ecosystem of software, protocols, and resources. For many, navigating this landscape can feel overwhelming.\nThat’s where the awesome-single-cell repository comes in.\n\nWhat It Is\n\n\n\nCurated by Sean Davis and a broad community of contributors, this open-source GitHub repository has become a central hub for single-cell researchers. It’s a living, community-driven catalog of tools and resources spanning the field—covering RNA-seq, ATAC-seq, epigenomics, spatial transcriptomics, and more.\nThe project’s popularity speaks for itself: over 3.5k stars and 1k forks on GitHub, with more than 180 contributors actively keeping it updated.\n\n\n\nWhy It Matters\nSingle-cell datasets bring unique analytical hurdles: correcting noise, integrating across batches or assays, identifying rare cell types, or charting developmental trajectories. Each task requires carefully tuned methods, and the awesome-single-cell repo provides a roadmap to solutions.\nFrom basics like quality control and dimension reduction to advanced workflows for pseudotime inference or gene regulatory network analysis, the repository organizes the community’s best tools in one place.\n\n\nWhat You’ll Find\nThe resource is carefully structured to meet the diverse needs of the field:\n\nSoftware Packages – Key tools for transcriptomics (e.g., Scanpy, Seurat), epigenomics (e.g., ArchR, Signac), copy number analysis, and the emerging wave of single-cell large models (e.g., scGPT).\nMulti-Omics Integration – Approaches like LIGER, TotalVI, and MultiVI for combining modalities such as RNA, ATAC, and protein.\nSpatial Transcriptomics – Tools like BayesSpace, SpaGCN, and CellTrek for mapping gene expression in tissue space.\nData Portals & Apps – Public datasets and interactive platforms, including CELLxGENE and the EBI Single Cell Atlas.\nTutorials & Workflows – End-to-end guides, from Bioconductor pipelines to nf-core workflows.\nKey Reviews & Comparisons – Curated literature to help benchmark and select the right methods.\n\n\n\nFinal Thoughts\n\nThe single-cell field is complex and fast-moving, but the awesome-single-cell repository acts as a compass, pointing researchers toward the right tools, datasets, and learning resources. It’s a prime example of how community collaboration can keep pace with scientific innovation—and make the journey a little less overwhelming."
  },
  {
    "objectID": "posts/04.Lying1/index.html",
    "href": "posts/04.Lying1/index.html",
    "title": "Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)",
    "section": "",
    "text": "So here’s the thing about analyzing gene expression data. You load up your Xenium or Visium dataset, you run the standard Seurat or SpatialData pipelines because that’s what everyone does, you get your clusters, and you write your paper. Great. Except maybe not great, because it turns out that the normalization step you copied from the single-cell RNA-seq tutorial might be quietly removing the very biological signal you’re trying to find.\nThis isn’t hypothetical hand-waving. Data from recent benchmarking studies tells a pretty clear story. When you apply standard single-cell normalization methods to spatial transcriptomics data, especially in tissues with anatomically structured variation in cell density or RNA content, you can inadvertently erase genuine biology. The cortical white matter doesn’t have uniform cell packing. The liver lobule has zones with different metabolic activities and different RNA abundances. These regional differences aren’t technical noise, they’re the biology. But if you normalize them away, they’re gone.\nLet’s unpack how we got here and what we can do about it."
  },
  {
    "objectID": "posts/04.Lying1/index.html#the-normalization-problem-what-were-actually-trying-to-fix",
    "href": "posts/04.Lying1/index.html#the-normalization-problem-what-were-actually-trying-to-fix",
    "title": "Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)",
    "section": "The Normalization Problem: What We’re Actually Trying to Fix",
    "text": "The Normalization Problem: What We’re Actually Trying to Fix\nSingle-cell RNA sequencing generates count matrices where each number represents detected mRNA molecules. The raw count for any gene in any cell reflects true expression plus a whole stack of technical artifacts. Cell lysis efficiency varies. mRNA capture rates fluctuate. Amplification introduces biases. Sequencing depth differs across cells. These technical factors create cell-specific and gene-specific effects that confound biological interpretation if you don’t correct for them.\nThe standard approach has been to compute some kind of size factor per cell and scale everything uniformly. LogNormalize in Seurat does exactly this. It divides each gene’s count by the total counts in that cell, multiplies by 10,000, and takes the log. The math is simple: log(count/total × 10000 + 1). The assumption is also simple: all genes scale uniformly with sequencing depth. This assumption works reasonably well for identifying cell types in peripheral blood mononuclear cells or dissociated tissues where cells are randomly distributed.\nHere’s where spatial data breaks that assumption. In spatial transcriptomics, cells aren’t randomly distributed. They’re organized into anatomical structures with systematic differences in cell density, cell types, and transcriptional activity. The white matter has fewer cells than gray matter. The portal zone of the liver lobule has different metabolic demands than the central vein region. These spatial gradients in total RNA content aren’t technical artifacts, they’re biology. When you normalize by total counts per spot or per cell, you’re assuming that variation in library size is purely technical. But what if it’s not?"
  },
  {
    "objectID": "posts/04.Lying1/index.html#from-simple-scaling-to-statistical-frameworks",
    "href": "posts/04.Lying1/index.html#from-simple-scaling-to-statistical-frameworks",
    "title": "Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)",
    "section": "From Simple Scaling to Statistical Frameworks",
    "text": "From Simple Scaling to Statistical Frameworks\nThe field has evolved from these simple scaling approaches toward methods that model the statistical properties of count data more carefully. I’ll walk through the major categories because understanding the underlying logic matters more than memorizing method names.\nGlobal scaling methods like LogNormalize, CPM (counts per million), and TPM (transcripts per million) compute a single size factor per cell and scale all counts uniformly. These work fine when the assumption holds that library size variation is mostly technical. TPM adds gene length correction, which matters for full-length protocols but not for 3’ end counting methods where one transcript generates one count regardless of length.\nPooling-based methods like scran take a different approach. Instead of computing size factors from individual cells, which fails when cells have lots of zeros, scran aggregates counts across pools of similar cells. This overcomes zero inflation by working with more robust pool-level totals before deconvolving back to cell-level factors. The method explicitly addresses the dropout problem where expressed transcripts fail to be captured.\nRegression-based methods model count data using generalized linear models with sequencing depth as a covariate. SCTransform represents the most widely adopted approach in this category. It fits a regularized negative binomial regression for each gene, modeling the mean-variance relationship and stabilizing variance across expression levels. The second version (SCTransform v2) improves handling of datasets with different sequencing depths across conditions, which reduces false positives in differential expression analysis.\nBayesian methods like BASiCS and SCDE use probabilistic frameworks to separate technical from biological variation. These approaches model dropout events explicitly and can distinguish biological zeros from technical failures. The trade-off is computational cost, which scales poorly to large datasets.\nDeep learning methods like scVI use variational autoencoders to learn low-dimensional representations while accounting for technical noise. The model learns to disentangle biological variation from batch effects and library size effects without requiring explicit statistical assumptions about the count distribution. These methods work particularly well for integration tasks where you’re combining multiple datasets.\nCount-preserving methods represent an alternative philosophy. Instead of transforming counts through normalization, methods like countland work directly with integer counts using models that naturally handle count data. This preserves the discrete nature of molecular counting and avoids potential artifacts from log transformations.\nEach approach makes different assumptions about the data generating process. No single method wins across all contexts."
  },
  {
    "objectID": "posts/04.Lying1/index.html#what-works-for-single-cell-rna-seq",
    "href": "posts/04.Lying1/index.html#what-works-for-single-cell-rna-seq",
    "title": "Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)",
    "section": "What Works for Single-Cell RNA-seq",
    "text": "What Works for Single-Cell RNA-seq\nBenchmarking studies have evaluated these methods across diverse datasets and analytical tasks. For standard single-cell RNA-seq analysis, particularly with UMI-based protocols like 10x Genomics, current best practices converge on a few options.\nSCTransform v2 performs well for clustering and differential expression, especially when comparing conditions with different sequencing depths. The variance stabilization it provides reduces the impact of mean-variance relationships that can create spurious patterns. Scran remains a solid choice, particularly for integration workflows where it’s been extensively validated. Recent work suggests Dino, which models the full distribution of counts for variance stabilization, performs particularly well for high-throughput clustering applications.\nThe key insight is that method choice depends on experimental context and analytical goals. Full-length protocols like Smart-seq2 benefit from gene length normalization that’s irrelevant for UMI data. Integration tasks may favor deep learning approaches that can handle complex batch structures. Differential expression testing requires methods that control false positive rates appropriately."
  },
  {
    "objectID": "posts/04.Lying1/index.html#where-spatial-data-breaks-the-rules",
    "href": "posts/04.Lying1/index.html#where-spatial-data-breaks-the-rules",
    "title": "Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)",
    "section": "Where Spatial Data Breaks the Rules",
    "text": "Where Spatial Data Breaks the Rules\nSpatial transcriptomics introduces a fundamental complication. The spatial association between region-specific library size and underlying biology means that standard normalization can remove genuine signal.\n\nConsider a Visium experiment profiling brain tissue. Cortical white matter has lower cell density and lower total RNA content than gray matter. This creates a spatial gradient in library size that corresponds to anatomical structure. If you apply standard LogNormalize, you’re implicitly treating this gradient as technical noise and normalizing it away. But the gradient is biology. Different brain regions have different cellular compositions and different transcriptional programs.\nThis problem becomes even more acute with subcellular resolution platforms like Xenium or CosMx. Individual cells have spatial organization of organelles, transcripts localize to specific subcellular compartments, and RNA abundance varies systematically across cellular domains. Normalizing by cell-level totals can obscure these patterns.\nRecent work from Svensson and colleagues demonstrated this problem empirically. They showed that applying standard single-cell normalization methods to spatial data can reduce performance in spatial domain identification tasks. In some datasets, unnormalized or minimally normalized data (just log transformation) outperformed aggressively normalized data for identifying anatomical regions.\nThe issue is that we borrowed tools designed for one problem (technical variation in dissociated single cells) and applied them to a different problem (spatial biology where location matters). The assumptions don’t transfer cleanly."
  },
  {
    "objectID": "posts/04.Lying1/index.html#practical-recommendations-what-to-actually-do",
    "href": "posts/04.Lying1/index.html#practical-recommendations-what-to-actually-do",
    "title": "Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)",
    "section": "Practical Recommendations: What to Actually Do",
    "text": "Practical Recommendations: What to Actually Do\nWhen analyzing spatial transcriptomics data, start by examining the raw data. Plot spatial distributions of total counts and key marker genes. Look for regional patterns. If you see systematic spatial variation that corresponds to known anatomical structures, be cautious about aggressive normalization.\nFor spatial domain identification, consider minimal normalization or log transformation without scaling. Recent benchmarks suggest this often works better than methods that remove spatial library size variation.\nIf you use standard methods like LogNormalize, validate your results against known biology. Marker genes with established spatial patterns provide internal validation. If your normalized data shows uniform expression of a gene that should be spatially restricted, your normalization likely removed signal.\nFor imaging-based platforms with defined gene panels, evaluate whether the panel composition introduces systematic biases. Consider area-based or volume-based normalization as alternatives to count-based methods, particularly for subcellular resolution data.\nSpaNorm provides a purpose-built solution for data where spatial library size variation reflects biology. The method is most valuable for tissues with strong anatomical organization and for subcellular resolution platforms.\nFor differential expression analysis in spatial data, pseudobulk approaches that aggregate spots within regions and apply bulk methods remain a robust choice. This sidesteps many normalization challenges by working at the region level rather than spot level."
  },
  {
    "objectID": "posts/04.Lying1/index.html#the-deeper-issue-technical-noise-versus-biological-signal",
    "href": "posts/04.Lying1/index.html#the-deeper-issue-technical-noise-versus-biological-signal",
    "title": "Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)",
    "section": "The Deeper Issue: Technical Noise Versus Biological Signal",
    "text": "The Deeper Issue: Technical Noise Versus Biological Signal\nThe normalization problem in spatial transcriptomics highlights a broader challenge in computational biology. We build tools to remove technical variation, but those tools encode assumptions about what counts as technical versus biological. When data structure violates those assumptions, our corrections become overcorrections.\nThis matters because spatial transcriptomics is moving toward higher resolution, greater multiplexing, and multimodal integration. Subcellular resolution platforms let us see transcript localization patterns. Multiplexed imaging adds protein measurements. These technologies generate data where spatial structure carries biological information that we need to preserve.\nThe solution isn’t to abandon normalization. Raw count data has real technical artifacts that need correction. The solution is to develop methods that respect spatial structure while removing noise. SpaNorm represents a first step. Methods that incorporate tissue architecture, cell type composition, and spatial autocorrelation will likely follow.\nFor multimodal data combining RNA and protein measurements, we need modality-specific approaches. Protein data from CITE-seq experiments shows different statistical properties than RNA counts. Methods like CLR (centered log ratio) and dsb explicitly address antibody-derived protein data characteristics. As technologies integrate more data types, normalization strategies will need corresponding sophistication."
  },
  {
    "objectID": "posts/04.Lying1/index.html#what-this-means-for-your-analysis",
    "href": "posts/04.Lying1/index.html#what-this-means-for-your-analysis",
    "title": "Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)",
    "section": "What This Means for Your Analysis",
    "text": "What This Means for Your Analysis\nIf you’re analyzing spatial transcriptomics data, the default pipeline might not be your friend. The standard workflow that works beautifully for PBMC single-cell data can quietly remove spatial biology from tissue data.\nBefore normalizing, understand what your method assumes. LogNormalize assumes uniform scaling relationships. SCTransform assumes mean-variance relationships follow a specific form. Deep learning methods make implicit assumptions through their architecture choices. If your data violates these assumptions, your normalization will distort your results.\nValidate against known biology. If you have marker genes with established spatial patterns, check that your normalized data preserves those patterns. If normalization removes them, you’ve overcorrected.\nConsider whether you need normalization for your specific analysis. For some spatial domain identification tasks, minimal processing works better than aggressive normalization. For differential expression, pseudobulk approaches offer robustness. For integration, spatially-aware methods matter.\nThe fundamental goal remains unchanged: distinguish technical variation from biological variation while preserving the latter. But in spatial data, where spatial variation is often biological, we need tools that respect that structure."
  },
  {
    "objectID": "posts/04.Lying1/index.html#looking-forward",
    "href": "posts/04.Lying1/index.html#looking-forward",
    "title": "Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)",
    "section": "Looking Forward",
    "text": "Looking Forward\nThe field is moving fast. Spatial transcriptomics technologies continue to improve in resolution and throughput. Normalization methods are adapting accordingly. Spatially-aware approaches will become more sophisticated as we better understand tissue architecture and spatial gene expression patterns.\nIntegration of multiple data modalities will require normalization methods that handle RNA, protein, metabolites, and morphological features simultaneously. Each modality has different noise characteristics and different scaling relationships.\nMachine learning approaches will likely play an increasing role. Methods that learn normalization strategies from data rather than imposing parametric assumptions could adapt to diverse experimental contexts. The challenge will be maintaining interpretability and biological validity.\nFor now, the practical advice is simple. Understand your tools. Validate your results. Don’t assume that methods designed for single-cell RNA-seq transfer directly to spatial data. The biology is different, the technical artifacts are different, and the normalization strategy should be different too.\nYour spatial transcriptomics data contains rich biological information about tissue organization, cellular interactions, and spatial gene expression patterns. Choose normalization methods that preserve that information rather than normalize it away."
  },
  {
    "objectID": "posts/04.Lying1/index.html#further-reading",
    "href": "posts/04.Lying1/index.html#further-reading",
    "title": "Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)",
    "section": "Further Reading",
    "text": "Further Reading\nFor those who want to dive deeper into the technical details, the key references include:\nOn standard normalization methods: - Hafemeister C, Satija R. Normalization and variance stabilization of single-cell RNA-seq data using regularized negative binomial regression. Genome Biology 2019. https://doi.org/10.1186/s13059-019-1874-1\n\nLun ATL, et al. Pooling across cells to normalize single-cell RNA sequencing data with many zero counts. Genome Biology 2016. https://doi.org/10.1186/s13059-016-0947-7\n\nOn spatial-specific challenges: - Svensson V, et al. (References to SpaNorm and spatial normalization challenges from the original report)\nOn deep learning approaches: - Lopez R, et al. Deep generative modeling for single-cell transcriptomics. Nature Methods 2018. https://doi.org/10.1038/s41592-018-0229-2\nOn benchmarking: - Brown J, et al. Normalization by distributional resampling of high throughput single-cell RNA-sequencing data. Bioinformatics 2021. https://doi.org/10.1093/bioinformatics/btab450 - Marco-Salas S, et al. Optimizing Xenium In Situ data utility by quality assessment and best-practice analysis workflows. Nature Methods 2025. https://doi.org/10.1038/s41592-025-02617-2"
  },
  {
    "objectID": "posts/04.Lying1/linkedin-newsletters.html",
    "href": "posts/04.Lying1/linkedin-newsletters.html",
    "title": "LinkedIn Newsletter Summaries for “Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)”",
    "section": "",
    "text": "Angle: Lead with the critical error researchers are making\nYour spatial transcriptomics pipeline just erased your most important findings.\nThe normalization step you copied from single-cell RNA-seq tutorials? It’s treating regional differences in cell density and RNA content as technical noise—and removing them. That cortical white matter gradient isn’t an artifact. The liver lobule zonation isn’t sequencing bias. It’s the biology you’re trying to study.\nHere’s what’s actually happening in your analysis:\n\nLogNormalize assumes library size variation is purely technical—but in spatial data, it often reflects anatomical structure and metabolic differences\nStandard scRNA-seq methods remove spatial gradients that correspond to genuine tissue organization (recent benchmarks show unnormalized data can outperform aggressively normalized data for spatial domain identification)\nThe tools we borrowed from dissociated single cells were never designed for spatially-organized tissues where location encodes biology\nSubcellular resolution platforms (Xenium, CosMx) make this worse—normalizing by cell-level totals can obscure transcript localization patterns\nYour marker genes with known spatial patterns should be your validation check—if normalization removes them, you’ve overcorrected\n\nThe solution isn’t abandoning normalization—raw data has real technical artifacts. It’s choosing methods that respect spatial structure. SpaNorm, minimal normalization for domain identification, and pseudobulk approaches for differential expression all preserve biology while removing noise.\nRead the full post for practical recommendations, method comparisons, and validation strategies that ensure your spatial analysis captures biology instead of removing it.\n\n\n\n\nAngle: Position as essential knowledge for computational biologists\nThe assumption breaking your spatial transcriptomics analysis: uniform scaling relationships.\nEvery normalization method encodes assumptions about your data. LogNormalize assumes all genes scale uniformly with sequencing depth. SCTransform assumes mean-variance relationships follow a specific form. These assumptions work beautifully for dissociated PBMCs—but systematically fail for tissues with anatomical organization.\nWhat changes when your data has spatial structure:\n\nRegional variation in total RNA isn’t technical noise—it’s organized biology (brain white matter has genuinely lower cell density and RNA content than gray matter)\nThe evolution from simple scaling to statistical frameworks matters: global scaling, pooling-based methods (scran), regression approaches (SCTransform v2), Bayesian frameworks, and deep learning each make different assumptions about the data generating process\nBenchmarking reveals method-task dependencies—SCTransform v2 excels for differential expression with depth variation, Dino for high-throughput clustering, but spatial domain identification often favors minimal normalization\nImaging-based platforms with defined gene panels introduce systematic biases that area-based or volume-based normalization can address better than count-based methods\nMultimodal integration (RNA + protein) requires modality-specific approaches—CLR and dsb explicitly handle antibody-derived protein data characteristics\n\nThe practical workflow: examine raw data first, plot spatial distributions of total counts, validate normalized results against known marker gene patterns, and choose methods appropriate for your specific analytical task.\nClick through for the complete technical breakdown, including when to use SpaNorm, how to validate against biological ground truth, and why pseudobulk approaches sidestep many normalization challenges entirely.\n\n\n\n\nAngle: Challenge conventional wisdom to drive curiosity\nThe best normalization for spatial transcriptomics data might be no normalization at all.\nCounterintuitive? Absolutely. Supported by recent benchmarks? Yes. In multiple spatial datasets, unnormalized data (or just log transformation) outperformed standard single-cell methods for identifying anatomical regions. We’ve been so focused on removing technical noise that we forgot to ask whether we’re also removing signal.\nThe fundamental problem we’re facing:\n\nWe borrowed tools from the wrong problem—methods designed for technical variation in randomly distributed dissociated cells are now being applied to spatially-organized tissues where location IS the biology\nSpatial library size variation correlates with anatomy (white matter vs. gray matter, portal zone vs. central vein), and treating it as noise removes the biological patterns you’re trying to discover\nRecent work from Svensson et al. demonstrates this empirically: standard scRNA-seq normalization reduces performance in spatial domain identification tasks\nHigher resolution makes this worse—subcellular platforms reveal transcript localization and organelle organization that cell-level normalization obscures\nThe correction becomes overcorrection when your tools can’t distinguish technical artifacts from biological spatial structure\n\nThis doesn’t mean raw counts are the answer for everything. Differential expression still needs proper statistical frameworks. Integration tasks benefit from sophisticated batch correction. But for spatial domain identification—the fundamental question of “what regions exist in my tissue”—aggressive normalization may hurt more than it helps.\nThe full article breaks down when to normalize aggressively, when to use minimal processing, and how to validate that your chosen method preserves biology. Because the goal isn’t perfect normalization—it’s distinguishing technical variation from biological variation while keeping the latter intact.\n\n\n\n\nEach version targets slightly different audience motivations: 1. Version 1: Appeals to researchers worried about making mistakes (fear of invalidating findings) 2. Version 2: Appeals to technical experts who value methodological rigor (credibility and depth) 3. Version 3: Appeals to contrarian thinkers and those questioning established practices (intellectual curiosity)\nAll three maintain scientific accuracy, create clear value, and drive clicks through genuine curiosity rather than clickbait."
  },
  {
    "objectID": "posts/04.Lying1/linkedin-newsletters.html#version-1-problem-focused-hook",
    "href": "posts/04.Lying1/linkedin-newsletters.html#version-1-problem-focused-hook",
    "title": "LinkedIn Newsletter Summaries for “Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)”",
    "section": "",
    "text": "Angle: Lead with the critical error researchers are making\nYour spatial transcriptomics pipeline just erased your most important findings.\nThe normalization step you copied from single-cell RNA-seq tutorials? It’s treating regional differences in cell density and RNA content as technical noise—and removing them. That cortical white matter gradient isn’t an artifact. The liver lobule zonation isn’t sequencing bias. It’s the biology you’re trying to study.\nHere’s what’s actually happening in your analysis:\n\nLogNormalize assumes library size variation is purely technical—but in spatial data, it often reflects anatomical structure and metabolic differences\nStandard scRNA-seq methods remove spatial gradients that correspond to genuine tissue organization (recent benchmarks show unnormalized data can outperform aggressively normalized data for spatial domain identification)\nThe tools we borrowed from dissociated single cells were never designed for spatially-organized tissues where location encodes biology\nSubcellular resolution platforms (Xenium, CosMx) make this worse—normalizing by cell-level totals can obscure transcript localization patterns\nYour marker genes with known spatial patterns should be your validation check—if normalization removes them, you’ve overcorrected\n\nThe solution isn’t abandoning normalization—raw data has real technical artifacts. It’s choosing methods that respect spatial structure. SpaNorm, minimal normalization for domain identification, and pseudobulk approaches for differential expression all preserve biology while removing noise.\nRead the full post for practical recommendations, method comparisons, and validation strategies that ensure your spatial analysis captures biology instead of removing it."
  },
  {
    "objectID": "posts/04.Lying1/linkedin-newsletters.html#version-2-technical-credibility-hook",
    "href": "posts/04.Lying1/linkedin-newsletters.html#version-2-technical-credibility-hook",
    "title": "LinkedIn Newsletter Summaries for “Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)”",
    "section": "",
    "text": "Angle: Position as essential knowledge for computational biologists\nThe assumption breaking your spatial transcriptomics analysis: uniform scaling relationships.\nEvery normalization method encodes assumptions about your data. LogNormalize assumes all genes scale uniformly with sequencing depth. SCTransform assumes mean-variance relationships follow a specific form. These assumptions work beautifully for dissociated PBMCs—but systematically fail for tissues with anatomical organization.\nWhat changes when your data has spatial structure:\n\nRegional variation in total RNA isn’t technical noise—it’s organized biology (brain white matter has genuinely lower cell density and RNA content than gray matter)\nThe evolution from simple scaling to statistical frameworks matters: global scaling, pooling-based methods (scran), regression approaches (SCTransform v2), Bayesian frameworks, and deep learning each make different assumptions about the data generating process\nBenchmarking reveals method-task dependencies—SCTransform v2 excels for differential expression with depth variation, Dino for high-throughput clustering, but spatial domain identification often favors minimal normalization\nImaging-based platforms with defined gene panels introduce systematic biases that area-based or volume-based normalization can address better than count-based methods\nMultimodal integration (RNA + protein) requires modality-specific approaches—CLR and dsb explicitly handle antibody-derived protein data characteristics\n\nThe practical workflow: examine raw data first, plot spatial distributions of total counts, validate normalized results against known marker gene patterns, and choose methods appropriate for your specific analytical task.\nClick through for the complete technical breakdown, including when to use SpaNorm, how to validate against biological ground truth, and why pseudobulk approaches sidestep many normalization challenges entirely."
  },
  {
    "objectID": "posts/04.Lying1/linkedin-newsletters.html#version-3-contrarianprovocative-hook",
    "href": "posts/04.Lying1/linkedin-newsletters.html#version-3-contrarianprovocative-hook",
    "title": "LinkedIn Newsletter Summaries for “Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)”",
    "section": "",
    "text": "Angle: Challenge conventional wisdom to drive curiosity\nThe best normalization for spatial transcriptomics data might be no normalization at all.\nCounterintuitive? Absolutely. Supported by recent benchmarks? Yes. In multiple spatial datasets, unnormalized data (or just log transformation) outperformed standard single-cell methods for identifying anatomical regions. We’ve been so focused on removing technical noise that we forgot to ask whether we’re also removing signal.\nThe fundamental problem we’re facing:\n\nWe borrowed tools from the wrong problem—methods designed for technical variation in randomly distributed dissociated cells are now being applied to spatially-organized tissues where location IS the biology\nSpatial library size variation correlates with anatomy (white matter vs. gray matter, portal zone vs. central vein), and treating it as noise removes the biological patterns you’re trying to discover\nRecent work from Svensson et al. demonstrates this empirically: standard scRNA-seq normalization reduces performance in spatial domain identification tasks\nHigher resolution makes this worse—subcellular platforms reveal transcript localization and organelle organization that cell-level normalization obscures\nThe correction becomes overcorrection when your tools can’t distinguish technical artifacts from biological spatial structure\n\nThis doesn’t mean raw counts are the answer for everything. Differential expression still needs proper statistical frameworks. Integration tasks benefit from sophisticated batch correction. But for spatial domain identification—the fundamental question of “what regions exist in my tissue”—aggressive normalization may hurt more than it helps.\nThe full article breaks down when to normalize aggressively, when to use minimal processing, and how to validate that your chosen method preserves biology. Because the goal isn’t perfect normalization—it’s distinguishing technical variation from biological variation while keeping the latter intact."
  },
  {
    "objectID": "posts/04.Lying1/linkedin-newsletters.html#usage-notes",
    "href": "posts/04.Lying1/linkedin-newsletters.html#usage-notes",
    "title": "LinkedIn Newsletter Summaries for “Why Your Spatial Transcriptomics Analysis Might Be Lying to You (I)”",
    "section": "",
    "text": "Each version targets slightly different audience motivations: 1. Version 1: Appeals to researchers worried about making mistakes (fear of invalidating findings) 2. Version 2: Appeals to technical experts who value methodological rigor (credibility and depth) 3. Version 3: Appeals to contrarian thinkers and those questioning established practices (intellectual curiosity)\nAll three maintain scientific accuracy, create clear value, and drive clicks through genuine curiosity rather than clickbait."
  },
  {
    "objectID": "posts/03.AIwriting/index.html",
    "href": "posts/03.AIwriting/index.html",
    "title": "AI-Enhanced Scientific Writing",
    "section": "",
    "text": "Quarto is an open-source scientific publishing system that enables researchers to weave together code, results, and narrative text into reproducible documents. Think of it as the evolution of R Markdown and Jupyter notebooks, a polyglot platform that works seamlessly with R, Python, Julia, and other languages.\nA Quarto project combines:\n\nSource files (.qmd or .ipynb): Plain-text documents containing code chunks, statistical analyses, and markdown text\nConfiguration (_quarto.yml): Defines the project structure, navigation, and visual theme\nRendered output (docs/ directory): Auto-generated HTML, PDF, or website versions of your analyses\n\nFor scientists, Quarto solves a critical problem: maintaining a single source of truth where code and interpretation live together, preventing the drift between analysis and write-up that plagues traditional workflows."
  },
  {
    "objectID": "posts/03.AIwriting/index.html#what-is-quarto",
    "href": "posts/03.AIwriting/index.html#what-is-quarto",
    "title": "AI-Enhanced Scientific Writing",
    "section": "",
    "text": "Quarto is an open-source scientific publishing system that enables researchers to weave together code, results, and narrative text into reproducible documents. Think of it as the evolution of R Markdown and Jupyter notebooks, a polyglot platform that works seamlessly with R, Python, Julia, and other languages.\nA Quarto project combines:\n\nSource files (.qmd or .ipynb): Plain-text documents containing code chunks, statistical analyses, and markdown text\nConfiguration (_quarto.yml): Defines the project structure, navigation, and visual theme\nRendered output (docs/ directory): Auto-generated HTML, PDF, or website versions of your analyses\n\nFor scientists, Quarto solves a critical problem: maintaining a single source of truth where code and interpretation live together, preventing the drift between analysis and write-up that plagues traditional workflows."
  },
  {
    "objectID": "posts/03.AIwriting/index.html#the-challenge-from-code-to-coherent-narrative",
    "href": "posts/03.AIwriting/index.html#the-challenge-from-code-to-coherent-narrative",
    "title": "AI-Enhanced Scientific Writing",
    "section": "The Challenge: From Code to Coherent Narrative",
    "text": "The Challenge: From Code to Coherent Narrative\nWhile Quarto excels at rendering code and results, transforming computational analysis into publication-ready scientific prose remains cognitively demanding. I normaly must:\n\nSynthesize complex operations into clear methodology descriptions\nExtract quantitative results from rendered outputs\nMaintain consistent scientific voice and style\nEnsure logical flow between analysis sections\nBalance technical accuracy with accessibility\n\nThis is where AI integration becomes transformative."
  },
  {
    "objectID": "posts/03.AIwriting/index.html#the-ai-enhanced-workflow-architecture",
    "href": "posts/03.AIwriting/index.html#the-ai-enhanced-workflow-architecture",
    "title": "AI-Enhanced Scientific Writing",
    "section": "The AI-Enhanced Workflow Architecture",
    "text": "The AI-Enhanced Workflow Architecture\n\nI have implemented a Claude-assisted scientific writing system in all my Quarto projects using Positron (IDE enviroment) built on three pillars:\n\n1. Project Instructions (CLAUDE.md)\nThe CLAUDE.md file serves as the command center, providing Claude with:\n\nSystem architecture overview: Explains the Positron IDE environment, Quarto structure, and file organization\nCoding standards: Enforces tidyverse patterns, vectorization over loops, use of native R pipe |&gt;, and mandatory code annotations\nWorkflow protocols: Defines how to interpret results, write/edit code, and handle git operations\nQuick reference guide: Maps common requests to specific actions\n\nThis file essentially “programs” Claude to function as a domain-specific research assistant who understands both the technical environment and scientific goals.\n\n\n2. Context Profiles (.context/ directory)\nThree critical files shape Claude’s behavior:\n\nquarto-narrative-skill/SKILL.md\nThis is the heart of the enhancer system; a comprehensive instruction manual for transforming Quarto analyses into scientific narratives. It defines:\n\nInput requirements: Reads three files per analysis:\n\nThe .qmd source (code logic and structure)\nThe rendered .html (numerical results and outputs)\nThe index.qmd (experimental context and research questions)\n\nCore workflow:\n\nParse all input files to extract code operations, results, and context\nMaintain exact section hierarchy from the .qmd structure\nFor each section: describe code operations → report HTML results → interpret biologically\nApply strict writing style constraints\nSynthesize into cohesive narrative prose\n\nHTML parsing strategy: Extracts content from rendered HTML by identifying text between tags (&lt;p&gt;, &lt;td&gt;, &lt;pre&gt;), ignoring JavaScript/CSS, and pulling numerical values from tables and code outputs\nSpecial handling: Covers multiple comparisons, negative results, technical issues, and complex visualizations\n\n\n\nwritingStyle_OSmithies.json\nA structured specification of scientific voice inspired by Nobel laureate Oliver Smithies. Key constraints:\n\nVoice: Active, first-person agency (“We performed” not “was performed”)\nTone: Decisive, objective, authoritative—avoid hedging\nSentence structure: Under 25 words, one idea per sentence, linear flow\nData reporting: Specific values, explicit statistical significance\nFlow pattern: Context → Action → Result → Meaning\n\nExample transformation:\n❌ \"Analysis was performed using DESeq2\"\n✅ \"We performed analysis using DESeq2\"\n\n❌ \"Many genes were significant\"\n✅ \"We detected 347 significant genes\"\n\n\ngit_workflow.md\nEnforces version control standards:\n\nCommit format: Title line + bullet points explaining specific changes\nDistinguishes between detailed explanations (for analysis code) and simple messages (for documentation)\nExplicitly excludes AI attribution messages\n\nIMPORTANT: These files live inside the Quarto project\n\n\n\n3. The Invocation Pattern\nWhen I need to document an analysis, I fire up Claude code in a terminal window and use the next prompt to trigger the workflow:\n\"Write the analysis for xxxxxx.qmd\" \nClaude then invokes quarto-narrative-skill:\n\nReads the source .qmd file (understanding methodology)\nReads the rendered .html file (extracting results)\nReads index.qmd (gathering experimental context)\nApplies the Oliver Smithies writing style\nGenerates publication-ready prose that:\n\nMaintains the exact section structure from the .qmd\nDescribes what each code chunk does and why\nReports specific numerical results from the HTML\nProvides biological interpretation\nUses active voice, concise sentences, and precise terminology"
  },
  {
    "objectID": "posts/03.AIwriting/index.html#example-output-pattern",
    "href": "posts/03.AIwriting/index.html#example-output-pattern",
    "title": "AI-Enhanced Scientific Writing",
    "section": "Example Output Pattern",
    "text": "Example Output Pattern\nGiven a .qmd section:\n## Differential Expression Analysis\ndds &lt;- DESeq(dds)\nres &lt;- results(dds, contrast=c(\"genotype\", \"APOE2\", \"WT\"))\nres_sig &lt;- subset(res, padj &lt; 0.05 & abs(log2FoldChange) &gt; 1)\nAnd HTML showing:\n347 genes with padj &lt; 0.05 and |log2FC| &gt; 1\nUpregulated: 198 genes\nDownregulated: 149 genes\nClaude generates: &gt; “We performed differential expression analysis using DESeq2 to compare APOE2 and wild-type mice. We identified 347 genes with adjusted p-values below 0.05 and absolute log2 fold changes exceeding 1. Of these, 198 genes showed increased expression in APOE2 mice, while 149 showed decreased expression.”"
  },
  {
    "objectID": "posts/03.AIwriting/index.html#why-this-approach-works",
    "href": "posts/03.AIwriting/index.html#why-this-approach-works",
    "title": "AI-Enhanced Scientific Writing",
    "section": "Why This Approach Works",
    "text": "Why This Approach Works\nThis system succeeds because it:\n\nSeparates concerns: Code lives in .qmd, results in .html, style rules in CLAUDE.md and .context/. Style rules can be reused in other projects.\nEnforces consistency: The writing style JSON ensures every narrative follows the same rigorous standard\nPreserves scientific accuracy: By reading both source code and rendered output, Claude reports exact values and methods\nMaintains structure: The skill explicitly preserves the .qmd section hierarchy, preventing AI “creativity” from reorganizing logical flow\nIs reproducible: The entire workflow is version-controlled and can be audited"
  },
  {
    "objectID": "posts/03.AIwriting/index.html#key-innovations",
    "href": "posts/03.AIwriting/index.html#key-innovations",
    "title": "AI-Enhanced Scientific Writing",
    "section": "Key Innovations",
    "text": "Key Innovations\n\nHTML-as-database: Treating rendered HTML as a structured data source for result extraction\nStyle-as-code: Formalizing scientific voice into a machine-readable specification\nContext injection: Using .context/ files to provide persistent behavioral constraints\nSkill-based invocation: Creating specialized “modes” for Claude through detailed skill definitions"
  },
  {
    "objectID": "posts/03.AIwriting/index.html#practical-benefits",
    "href": "posts/03.AIwriting/index.html#practical-benefits",
    "title": "AI-Enhanced Scientific Writing",
    "section": "Practical Benefits",
    "text": "Practical Benefits\nFor researchers, this system:\n\nReduces writing time from hours to minutes\nEnsures consistent voice across multi-year projects\nCatches methodology/results mismatches (if code doesn’t match description, it’s obvious)\nEnables rapid iteration (change analysis → re-render → regenerate narrative)\nCreates an audit trail (git history shows when code changed vs. when prose changed)"
  },
  {
    "objectID": "posts/03.AIwriting/index.html#files-that-make-it-work",
    "href": "posts/03.AIwriting/index.html#files-that-make-it-work",
    "title": "AI-Enhanced Scientific Writing",
    "section": "Files That Make It Work",
    "text": "Files That Make It Work\nEssential components:\n\nCLAUDE.md: System instructions and quick reference\n.context/quarto-narrative-skill/SKILL.md: Narrative generation protocol\n.context/writingStyle_OSmithies.json: Voice specification\n.context/git_workflow.md: Version control standards\n_quarto.yml: Project configuration\n\nTogether, these files transform Claude from a general-purpose AI into a specialized scientific writing assistant that understands computational biology, R/Python code, statistical analysis, and publication standards."
  },
  {
    "objectID": "posts/03.AIwriting/index.html#the-future-of-ai-assisted-science",
    "href": "posts/03.AIwriting/index.html#the-future-of-ai-assisted-science",
    "title": "AI-Enhanced Scientific Writing",
    "section": "The Future of AI-Assisted Science",
    "text": "The Future of AI-Assisted Science\n\nThis method demonstrates that AI’s value in research isn’t just running analyses, it’s in maintaining the connective tissue between computation and communication. By codifying scientific writing standards and creating structured workflows, we can leverage AI to handle the mechanical aspects of prose generation while researchers focus on interpretation and discovery.\n\n\nThe system is transparent, auditable, and version-controlled, essential properties for scientific reproducibility. It doesn’t replace scientific thinking; it amplifies it by removing friction between “I analyzed this” and “I can clearly explain what I did.”"
  },
  {
    "objectID": "posts/00.hello_world/index.html",
    "href": "posts/00.hello_world/index.html",
    "title": "Hello World…",
    "section": "",
    "text": "Welcome to our GitHub Pages site dedicated to omic sciences! 🧬🔬\nThis space will serve as a hub for sharing insights, tutorials, and resources related to:\n\nGenomics - DNA sequencing and analysis\nTranscriptomics - RNA expression studies\n\nProteomics - Protein identification and quantification\nMetabolomics - Metabolic pathway analysis\nBioinformatics - Computational tools and pipelines\n\n\n\nIn upcoming posts, we’ll cover:\n\nBest practices for omic data analysis\nTutorials on common bioinformatics tools\nReproducible research workflows\nData visualization techniques\nLatest advancements in multi-omics integration"
  },
  {
    "objectID": "posts/00.hello_world/index.html#what-to-expect",
    "href": "posts/00.hello_world/index.html#what-to-expect",
    "title": "Hello World…",
    "section": "",
    "text": "In upcoming posts, we’ll cover:\n\nBest practices for omic data analysis\nTutorials on common bioinformatics tools\nReproducible research workflows\nData visualization techniques\nLatest advancements in multi-omics integration"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog explores the intersection of omics, spatial biology, and data science. Here I will share notes, tutorials, and reflections from my work, along with occasional digressions, documenting my learning process in the open.\nMy goal is to create a practical resource for fellow researchers and students. I hope you find something that sparks an idea or helps solve a problem in your own projects.\nThis blog is a component of my research. For a comprehensive overview of our lab’s projects, publications, and collaborations, please visit our lab’s main page."
  }
]